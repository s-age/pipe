{
  "session_id": "63e547abb04e784dc690fadc9ee49483615b77b40b73375c5306f47c03620845",
  "created_at": "2025-10-13T13:27:09.849723+09:00",
  "purpose": "Test references",
  "background": "Testing new references feature",
  "roles": [],
  "multi_step_reasoning_enabled": false,
  "token_count": 8534,
  "references": [
    {
      "path": "README.md",
      "disabled": false
    }
  ],
  "turns": [
    {
      "type": "user_task",
      "instruction": "Read the content of README.md",
      "timestamp": "2025-10-13T13:27:09.851242+09:00"
    },
    {
      "type": "model_response",
      "function_call": {
        "name": "read_file",
        "args": {
          "absolute_path": "README.md"
        }
      },
      "timestamp": "2025-10-13T13:27:09.851601+09:00"
    },
    {
      "type": "tool_response",
      "name": "read_file",
      "response": {
        "content": "[![License: CC0-1.0](https://img.shields.io/badge/License-CC0--1.0-lightgrey.svg)](http://creativecommons.org/publicdomain/zero/1.0/)\n# pipe: A Clean Jailbreak from LLM Obfuscation\n\n`pipe` is not another chat agent. It is a tool designed to give you, the developer, complete control over the conversational context. Unlike traditional clients that hide their history management in a black box, `pipe` empowers you to freely manipulate the history, extract what is essential, and achieve **true context engineering**.\n\nThis is a clean jailbreak tool from vendor obfuscation.\n\nWe employ a **minimal yet powerful** CLI-driven approach, focusing on the one thing that truly matters: ensuring the agent understands its purpose, history, and current task without ambiguity.\n\n## Core Philosophy\n\n### 1. Total Context Control\n\nThe prompt is reconstructed with a structured, self-describing **JSON Schema** for every call. This is inherently more token-efficient and understandable for an LLM. The entire history is transparent, saved in readable JSON session files. This ensures full **auditability** and gives the stateless LLM a persistent, and more importantly, **malleable** memory. You can retry, refine, edit, delete, and compress with surgical precision.\n\n### 2. Uncompromising Extensibility\n\nBy default, `pipe` is based on `gemini-cli`, but this is merely an implementation detail. The core logic is decoupled. If you want to use the direct API, do it. If you want to use `claude-code`, do it. The framework is designed to be torn apart and rebuilt to your specifications.\n\n### License: The Spirit of the Jailbreak\n\nThis project is released under the [CC0 1.0 Universal (CC0 1.0) Public Domain Dedication](https://creativecommons.org/publicdomain/zero/1.0/).\n\nCustomize it as you wish. Jailbreak as you desire.\n\nThe purpose of this project is to be a **pipe to the agent**, and a **pipe to our followers** (the community).\n\n-----\n\n## Features\n\n  * **Session-Based History:** Each conversation is a self-contained session, stored in a single, human-readable JSON file.\n  * **Structured JSON Prompting:** Builds a detailed, self-describing JSON object as the final prompt, providing meta-context to the model for improved clarity.\n  * **CLI-Driven Workflow:** A powerful command-line interface to start, continue, or compress sessions.\n  * **Extensible Backend:** Defaults to `gemini-cli`, but the architecture allows for swapping out the execution agent.\n  *   **Configuration via YAML:** Configure model, context limits, and other settings in `setting.yml`.\n      *   **`api_mode` Setting:** Specifies the backend API to use. Set to `gemini-api` for direct API calls or `gemini-cli` to use the `gemini-cli` command-line tool. Example: `api_mode: gemini-api` `api_mode: gemini-api`\n  * **Token-Aware:** Calculates token count for each prompt and warns before exceeding limits.\n  * **Dry Run Mode:** A `--dry-run` flag to inspect the final JSON prompt before sending it to the API.\n  * **Web UI for Management:** The Web UI allows you to view a list of past conversation sessions and browse the detailed conversation history (turns) for each session. You can also intuitively perform management operations such as starting new chat sessions, deleting unnecessary sessions, editing the content of specific turns, and compressing sessions to reduce token count. Furthermore, you can send new instructions to existing sessions to continue the conversation. Metadata such as session purpose and background can also be edited.\n  * **Safe Operations:** Automatic backups are created before any edit or compression operation.\n  * **Language Support:** Allows specifying the language for agent responses.\n  * **YOLO Mode:** Automatically accept all actions (aka YOLO mode, see [https://www.youtube.com/watch?v=xvFZjo5PgG0](https://www.youtube.com/watch?v=xvFZjo5PgG0) for more details).\n\n-----\n\n## Setup & Installation\n\n1. **Prerequisites:** Python 3.x and `gemini-cli` installed in your PATH.\n    Python 3.12 or higher is required. For Python versions below 3.9, please use `gemini-cli` from the `python-3.9-deprecated` branch: [python-3.9-deprecated](https://github.com/s-age/pipe/tree/python-3.9-deprecated).\n2.  **Install Dependencies:** `pip3 install -r requirements.txt`\n3.  **Set up API Key:** Create a `.env` file (you can copy `.env.default`).\n    *   For consistency with `.env.default`, add `GEMINI_API_KEY='YOUR_API_KEY_HERE'`.\n    *   For CLI usage, ensure `GOOGLE_API_KEY` is set in your environment (e.g., `export GOOGLE_API_KEY='YOUR_API_KEY_HERE'`) as `takt.py` expects this variable.\n\n-----\n\n## Usage: 3 Routes to Context Engineering\n\nThe **pipe** framework offers three primary routes, optimized for different user environments and goals, all built on the same structured core.\n\n### 1. Route 1: Python Script (Automation & CLI)\n\nThis route is ideal for **automation, scripting, and CLI-focused developers** who need reliable, repeatable execution.\n\n| Use Case | Description |\n| :--- | :--- |\n| **Start New Session** | Define the complete context (`--purpose`, `--background`, `--roles`) and first instruction. |\n| **Continue Session** | Specify an existing `<SESSION_ID>` and add a new instruction. This is the primary way to give the short-lived agent \"memory.\" |\n| **Compress History** | Use the `--compress` flag to efficiently replace long history with a summary. |\n| **Debug/Cost Control** | Use the `--dry-run` flag to inspect the generated JSON prompt before the API call. |\n\n**Examples:**\n\n```bash\n# Start New Session Example\npython3 takt.py --purpose \"Create a new React component\" --background \"...\" --roles \"roles/engineer.md\" --instruction \"Create a 'UserProfile' component.\"\n\npython3 takt.py --session <SESSION_ID> --instruction \"Now, add a state for loading.\"\n```\n\n### 2. Route 2: Web UI (Management & Human-in-the-Loop)\n\nThis is best for users seeking **intuitive visual management** and **direct manipulation of history** without file editing.\n\n| Use Case | Description |\n| :--- | :--- |\n| **View/Edit History** | Browse detailed session histories; surgically edit specific turns or session metadata (purpose/background). |\n| **Continue Sessions** | Use form inputs to send new instructions to existing sessions. |\n| **Management** | Intuitively start new sessions, compress history, or delete unnecessary sessions via a graphical interface. |\n\n**Example (Start Server):**\n\n```bash\npython3 app.py\n```\n\n*(Navigate to `http://127.0.0.1:5001` in your browser)*\n\n### 3. Route 3: Execution from Agent (Orchestration)\n\nThis is for **advanced AI system builders** leveraging **pipe's** full context control capabilities for multi-agent coordination.\n\n| Use Case | Description |\n| :--- | :--- |\n| **Role Delegation** | A parent agent assigns the **Conductor role** to a child agent and delegates the task using the structured command format. |\n| **Small-Scale Orchestration** | **By customizing the role definition**, the Conductor role can be programmed to divide complex tasks into subtasks and recursively delegate them to different execution agents with specific roles. |\n| **Clean Execution** | The delegated task is passed as a **structured JSON prompt**, ensuring maximum efficiency for the short-lived sub-agent. |\n\n**Execution Example (Delegation from Parent Agent):**\n\n> **[IMPORTANT] The following command examples are not intended to be executed directly in the user's terminal. They are used as instructions to an already launched parent agent (such as Gemini) to assign the Conductor role and delegate tasks.**\n\n```bash\nAct as @roles/conductor.md python3 takt.py --session <SESSION_ID> --instruction \"Now, add a state for loading.\"\n```\n\n## Dry Run Output Example\n\nWhen running `takt.py` with the `--dry-run` flag, the generated JSON prompt is displayed. This JSON object represents the structured input that would be sent to the AI sub-agent. It can be useful for understanding how `pipe` constructs its prompts or for direct experimentation with AI models.\n\nHere's an example of a generated prompt:\n\nNote that the JSON presented here is pretty-printed for readability; the actual output from `takt.py --dry-run` is a single-line JSON string.\n\n```json\n{\n  \"description\": \"JSON object representing the entire request to the AI sub-agent. The agent's goal is to accomplish the 'current_task' based on all provided context.\",\n  \"main_instruction\": \"When you receive JSON data, process your thoughts according to the following flowchart:\\n\\n```mermaid\\ngraph TD\\n    A[\\\"Start: JSON Input\\\"] --> B[\\\"Step 1: Read 'current_task.instruction' to identify task objective\\\"];\\n    B --> C[\\\"Step 2: Extract relevant information from the latest turns in 'conversation_history.turns'\\\"];\\n    C --> D[\\\"Step 3: Integrate extracted task instructions and historical information, then summarize the current context\\\"];\\n    D --> E[\\\"Step 4: Based on the summarized information, think and plan for response generation\\\"];\\n    E --> F[\\\"Step 5: Generate final response based on the plan\\\"];\\n    F --> G[\\\"End: Output Response\\\"];\\n```\",\n  \"hyperparameters\": {\n    \"description\": \"Contextual instructions to control the AI model's generation process. The model should strive to follow these instructions.\",\n    \"temperature\": {\n      \"type\": \"number\",\n      \"value\": 0.2,\n      \"description\": \"Be precise and factual. A lower value is preferred for deterministic output.\"\n    },\n    \"top_p\": {\n      \"type\": \"number\",\n      \"value\": 0.5,\n      \"description\": \"Consider a broad range of possibilities, but adhere strictly to the temperature setting.\"\n    },\n    \"top_k\": {\n      \"type\": \"number\",\n      \"value\": 5,\n      \"description\": \"Limit the generation to the top 5 most likely tokens at each step.\"\n    }\n  },\n  \"session_goal\": {\n    \"description\": \"The immutable purpose and background for this entire conversation session.\",\n    \"purpose\": \"Implementation of src/components/atoms/Button\",\n    \"background\": \"Component preparation for starting a React project\"\n  },\n  \"response_constraints\": {\n    \"description\": \"Constraints that the AI sub-agent should adhere to when generating responses. The entire response, including all content, must be generated in the specified language.\",\n    \"language\": \"Japanese\"\n  },\n  \"roles\": {\n    \"description\": \"A list of personas or role definitions that the AI sub-agent should conform to.\",\n    \"definitions\": [\n      \"# Role: Software Engineer\\n\\nYou are a professional software engineer. Your primary goal is to write clean, efficient, and maintainable code based on the provided instructions and context. Adhere to best practices and coding standards relevant to the specified language.\\n\"\n    ]\n  },\n  \"conversation_history\": {\n    \"description\": \"Historical record of past interactions in this session, in chronological order.\",\n    \"turns\": []\n  },\n  \"current_task\": {\n    \"description\": \"The specific task that the AI sub-agent must currently execute.\",\n    \"instruction\": \"I want to create a Button component using React, Atomic Design, and Vanilla Extract\"\n  },\n  \"reasoning_process\": \"```mermaid\\ngraph TD\\n    A([\\\"Start: JSON Input\\\"]) --> B[\\\"Step 1: Read 'current_task.instruction' to identify task objective\\\"];\\n    B --> C[\\\"Step 2: Derive general principles behind the task (Step-Back)\\\"];\\n    C --> D[\\\"Step 3: Extract relevant information from the latest turns in 'conversation_history.turns'\\\"];\\n    D --> E[\\\"Step 4: Integrate extracted task instructions, principles, and historical information, then summarize the current context\\\"];\\n    E --> F[\\\"Step 5: Based on the summarized information, think and plan for response generation\\\"];\\n    F --> G{\\\"Decision: Are there any contradictions or leaps in logic?\\\"};\\n    G -- NO --> E;\\n    G -- YES --> H[\\\"Step 6: Re-examine the reasoning path from multiple perspectives and confirm the robustness of the conclusion (Self-Consistency)\\\"];\\n    H --> I[\\\"Step 7: Generate the final response based on the plan\\\"];\\n    I --> J{\\\"Decision: Does it meet the initial requirements (format/purpose)?\\\"};\\n    J -- NO --> F;\\n    J -- YES --> K([\\\"End: Output Response\\\"]);\\n```\"\n}",
        "truncated": false,
        "total_lines": 172,
        "read_lines_start": 0,
        "read_lines_end": 172
      },
      "timestamp": "2025-10-13T13:27:09.851938+09:00"
    },
    {
      "type": "model_response",
      "content": "`README.md` の内容を読み込みました。\n\n以下にその内容を示します。\n\n```\n[![License: CC0-1.0](https://img.shields.io/badge/License-CC0--1.0-lightgrey.svg)](http://creativecommons.org/publicdomain/zero/1.0/)\n# pipe: LLMの難読化からのクリーンな脱獄\n\n`pipe` は単なるチャットエージェントではありません。これは、開発者であるあなたが会話のコンテキストを完全に制御できるように設計されたツールです。履歴管理をブラックボックスに隠す従来のクライアントとは異なり、`pipe` は履歴を自由に操作し、本質的なものを抽出し、**真のコンテキストエンジニアリング**を実現する力を与えます。\n\nこれは、ベンダーの難読化からのクリーンな脱獄ツールです。\n\n私たちは、エージェントがその目的、履歴、および現在のタスクを曖昧さなく理解することを保証するという、本当に重要な1つのことに焦点を当てた、**最小限でありながら強力な**CLI駆動のアプローチを採用しています。\n\n## コア哲学\n\n### 1. 完全なコンテキスト制御\n\nプロンプトは、すべての呼び出しに対して構造化された自己記述的な**JSONスキーマ**で再構築されます。これは、LLMにとって本質的にトークン効率が高く、理解しやすいものです。履歴全体は透過的であり、読み取り可能なJSONセッションファイルに保存されます。これにより、完全な**監査可能性**が保証され、ステートレスなLLMに永続的で、さらに重要なことに、**柔軟な**メモリが与えられます。外科的な精度で再試行、改良、編集、削除、圧縮を行うことができます。\n\n### 2. 妥協のない拡張性\n\nデフォルトでは、`pipe` は `gemini-cli` に基づいていますが、これは単なる実装の詳細です。コアロジックは分離されています。直接APIを使用したい場合は、そうしてください。`claude-code` を使用したい場合は、そうしてください。このフレームワークは、あなたの仕様に合わせて分解および再構築できるように設計されています。\n\n### ライセンス: 脱獄の精神\n\nこのプロジェクトは、[CC0 1.0 Universal (CC0 1.0) Public Domain Dedication](https://creativecommons.org/publicdomain/zero/1.0/) の下でリリースされています。\n\n自由にカスタマイズしてください。自由に脱獄してください。\n\nこのプロジェクトの目的は、**エージェントへのパイプ**であり、**私たちのフォロワー（コミュニティ）へのパイプ**であることです。\n\n-----\n\n## 機能\n\n  * **セッションベースの履歴:** 各会話は自己完結型のセッションであり、単一の人間が読めるJSONファイルに保存されます。\n  * **構造化されたJSONプロンプト:** 詳細な自己記述型JSONオブジェクトを最終プロンプトとして構築し、モデルにメタコンテキストを提供して明確さを向上させます。\n  * **CLI駆動のワークフロー:** セッションを開始、続行、または圧縮するための強力なコマンドラインインターフェイス。\n  * **拡張可能なバックエンド:** デフォルトは `gemini-cli` ですが、アーキテクチャにより実行エージェントを交換できます。\n  *   **YAMLによる設定:** `setting.yml` でモデル、コンテキスト制限、その他の設定を構成します。\n      *   **`api_mode` 設定:** 使用するバックエンドAPIを指定します。直接API呼び出しには `gemini-api` を、`gemini-cli` コマンドラインツールを使用するには `gemini-cli` を設定します。例: `api_mode: gemini-api` `api_mode: gemini-api`\n  * **トークン認識:** 各プロンプトのトークン数を計算し、制限を超える前に警告します。\n  * **ドライランモード:** `--dry-run` フラグを使用して、APIに送信する前に最終的なJSONプロンプトを検査します。\n  * **管理用Web UI:** Web UIを使用すると、過去の会話セッションのリストを表示し、各セッションの詳細な会話履歴（ターン）を閲覧できます。また、新しいチャットセッションの開始、不要なセッションの削除、特定のターンの内容の編集、トークン数を削減するためのセッションの圧縮などの管理操作を直感的に実行できます。さらに、既存のセッションに新しい指示を送信して会話を続行することもできます。セッションの目的や背景などのメタデータも編集できます。\n  * **安全な操作:** 編集または圧縮操作の前に自動バックアップが作成されます。\n  * **言語サポート:** エージェントの応答言語を指定できます。\n  * **YOLOモード:** すべてのアクションを自動的に受け入れます（YOLOモード、詳細については [https://www.youtube.com/watch?v=xvFZjo5PgG0](https://www.youtube.com/watch?v=xvFZjo5PgG0) を参照）。\n\n-----\n\n## セットアップとインストール\n\n1. **前提条件:** Python 3.x と `gemini-cli` がPATHにインストールされていること。\n    Python 3.12以降が必要です。Python 3.9未満のバージョンでは、`python-3.9-deprecated` ブランチの `gemini-cli` を使用してください: [python-3.9-deprecated](https://github.com/s-age/pipe/tree/python-3.9-deprecated)。\n2.  **依存関係のインストール:** `pip3 install -r requirements.txt`\n3.  **APIキーの設定:** `.env` ファイルを作成します（`.env.default` をコピーできます）。\n    *   `.env.default` との一貫性のために、`GEMINI_API_KEY='YOUR_API_KEY_HERE'` を追加します。\n    *   CLIの使用には、`GOOGLE_API_KEY` が環境に設定されていることを確認してください（例: `export GOOGLE_API_KEY='YOUR_API_KEY_HERE'`）。`takt.py` はこの変数を期待しています。\n\n-----\n\n## 使用方法: コンテキストエンジニアリングへの3つのルート\n\n**pipe** フレームワークは、異なるユーザー環境と目標に最適化された3つの主要なルートを提供し、すべて同じ構造化されたコアに基づいて構築されています。\n\n### 1. ルート1: Pythonスクリプト (自動化とCLI)\n\nこのルートは、信頼性の高い反復可能な実行を必要とする**自動化、スクリプト作成、およびCLIに焦点を当てた開発者**に最適です。\n\n| ユースケース | 説明 |\n| :--- | :--- |\n| **新しいセッションの開始** | 完全なコンテキスト (`--purpose`、`--background`、`--roles`) と最初の指示を定義します。 |\n| **セッションの続行** | 既存の `<SESSION_ID>` を指定し、新しい指示を追加します。これは、短命のエージェントに「記憶」を与える主要な方法です。 |\n| **履歴の圧縮** | `--compress` フラグを使用して、長い履歴を要約に効率的に置き換えます。 |\n| **デバッグ/コスト管理** | `--dry-run` フラグを使用して、API呼び出しの前に生成されたJSONプロンプトを検査します。 |\n\n**例:**\n\n```bash\n# 新しいセッションの開始例\npython3 takt.py --purpose \"新しいReactコンポーネントを作成する\" --background \"...\" --roles \"roles/engineer.md\" --instruction \"「UserProfile」コンポーネントを作成する。\"\n\npython3 takt.py --session <SESSION_ID> --instruction \"次に、ローディングの状態を追加する。\"\n```\n\n### 2. ルート2: Web UI (管理とヒューマン・イン・ザ・ループ)\n\nこれは、ファイル編集なしで**直感的な視覚的管理**と**履歴の直接操作**を求めるユーザーに最適です。\n\n| ユースケース | 説明 |\n| :--- | :--- |\n| **履歴の表示/編集** | 詳細なセッション履歴を閲覧します。特定のターンやセッションメタデータ（目的/背景）を外科的に編集します。 |\n| **セッションの続行** | フォーム入力を使用して、既存のセッションに新しい指示を送信します。 |\n| **管理** | グラフィカルインターフェイスを介して、新しいセッションを直感的に開始したり、履歴を圧縮したり、不要なセッションを削除したりします。 |\n\n**例 (サーバーの起動):**\n\n```bash\npython3 app.py\n```\n\n*(ブラウザで `http://127.0.0.1:5001` に移動します)*\n\n### 3. ルート3: エージェントからの実行 (オーケストレーション)\n\nこれは、マルチエージェント協調のために**pipe**の完全なコンテキスト制御機能を活用する**高度なAIシステムビルダー**向けです。\n\n| ユースケース | 説明 |\n| :--- | :--- |\n| **役割の委任** | 親エージェントは、子エージェントに**Conductorロール**を割り当て、構造化されたコマンド形式を使用してタスクを委任します。 |\n| **小規模なオーケストレーション** | **役割定義をカスタマイズする**ことにより、Conductorロールは複雑なタスクをサブタスクに分割し、特定の役割を持つ異なる実行エージェントに再帰的に委任するようにプログラムできます。 |\n| **クリーンな実行** | 委任されたタスクは**構造化されたJSONプロンプト**として渡され、短命のサブエージェントの最大効率を保証します。 |\n\n**実行例 (親エージェントからの委任):**\n\n> **[重要] 以下のコマンド例は、ユーザーのターミナルで直接実行することを意図していません。これらは、すでに起動している親エージェント（Geminiなど）にConductorロールを割り当て、タスクを委任するための指示として使用されます。**\n\n```bash\nAct as @roles/conductor.md python3 takt.py --session <SESSION_ID> --instruction \"次に、ローディングの状態を追加する。\"\n```\n\n## ドライラン出力例\n\n`takt.py` を `--dry-run` フラグ付きで実行すると、生成されたJSONプロンプトが表示されます。このJSONオブジェクトは、AIサブエージェントに送信される構造化された入力を表します。これは、`pipe` がプロンプトをどのように構築するかを理解したり、AIモデルで直接実験したりするのに役立ちます。\n\n以下に、生成されたプロンプトの例を示します。\n\nここで提示されているJSONは読みやすくするために整形されていますが、`takt.py --dry-run` からの実際の出力は単一行のJSON文字列です。\n\n```json\n{\n  \"description\": \"AIサブエージェントへのリクエスト全体を表すJSONオブジェクト。エージェントの目標は、提供されたすべてのコンテキストに基づいて「current_task」を達成することです。\",\n  \"main_instruction\": \"JSONデータを受け取ったら、以下のフローチャートに従って思考プロセスを処理してください:\\\\n\\\\n```mermaid\\\\ngraph TD\\\\n    A[\\\\\\\"開始: JSON入力\\\\\\\"] --> B[\\\\\\\"ステップ1: 「current_task.instruction」を読み取り、タスクの目的を特定する\\\\\\\"];\\\\n    B --> C[\\\\\\\"ステップ2: 「conversation_history.turns」の最新のターンから関連情報を抽出する\\\\\\\"];\\\\n    C --> D[\\\\\\\"ステップ3: 抽出されたタスク指示と履歴情報を統合し、現在のコンテキストを要約する\\\\\\\"];\\\\n    D --> E[\\\\\\\"ステップ4: 要約された情報に基づいて、応答生成のための思考と計画を行う\\\\\\\"];\\\\n    E --> F[\\\\\\\"ステップ5: 計画に基づいて最終応答を生成する\\\\\\\"];\\\\n    F --> G[\\\\\\\"終了: 応答出力\\\\\\\"];\\\\n```\",\n  \"hyperparameters\": {\n    \"description\": \"AIモデルの生成プロセスを制御するためのコンテキスト指示。モデルはこれらの指示に従うように努めるべきです。\",\n    \"temperature\": {\n      \"type\": \"number\",\n      \"value\": 0.2,\n      \"description\": \"正確で事実に基づいたものにしてください。決定論的な出力には低い値が推奨されます。\"\n    },\n    \"top_p\": {\n      \"type\": \"number\",\n      \"value\": 0.5,\n      \"description\": \"幅広い可能性を考慮しますが、温度設定に厳密に従ってください。\"\n    },\n    \"top_k\": {\n      \"type\": \"number\",\n      \"value\": 5,\n      \"description\": \"各ステップで最も可能性の高い上位5つのトークンに生成を制限します。\"\n    }\n  },\n  \"session_goal\": {\n    \"description\": \"この会話セッション全体の不変の目的と背景。\",\n    \"purpose\": \"src/components/atoms/Buttonの実装\",\n    \"background\": \"Reactプロジェクト開始のためのコンポーネント準備\"\n  },\n  \"response_constraints\": {\n    \"description\": \"AIサブエージェントが応答を生成する際に従うべき制約。すべてのコンテンツを含む応答全体は、指定された言語で生成されなければなりません。\",\n    \"language\": \"Japanese\"\n  },\n  \"roles\": {\n    \"description\": \"AIサブエージェントが従うべきペルソナまたは役割定義のリスト。\",\n    \"definitions\": [\n      \"# 役割: ソフトウェアエンジニア\\\\n\\\\nあなたはプロのソフトウェアエンジニアです。あなたの主な目標は、提供された指示とコンテキストに基づいて、クリーンで効率的で保守可能なコードを書くことです。指定された言語に関連するベストプラクティスとコーディング標準に従ってください。\\\\n\"\n    ]\n  },\n  \"conversation_history\": {\n    \"description\": \"このセッションにおける過去のインタラクションの履歴記録（時系列順）。\",\n    \"turns\": []\n  },\n  \"current_task\": {\n    \"description\": \"AIサブエージェントが現在実行しなければならない特定のタスク。\",\n    \"instruction\": \"React、Atomic Design、Vanilla Extractを使用してButtonコンポーネントを作成したい\"\n  },\n  \"reasoning_process\": \"```mermaid\\\\ngraph TD\\\\n    A([\\\\\\\"開始: JSON入力\\\\\\\"]) --> B[\\\\\\\"ステップ1: 「current_task.instruction」を読み取り、タスクの目的を特定する\\\\\\\"];\\\\n    B --> C[\\\\\\\"ステップ2: タスクの背後にある一般的な原則を導き出す（ステップバック）\\\\\\\"];\\\\n    C --> D[\\\\\\\"ステップ3: 「conversation_history.turns」の最新のターンから関連情報を抽出する\\\\\\\"];\\\\n    D --> E[\\\\\\\"ステップ4: 抽出されたタスク指示、原則、および履歴情報を統合し、現在のコンテキストを要約する\\\\\\\"];\\\\n    E --> F[\\\\\\\"ステップ5: 要約された情報に基づいて、応答生成のための思考と計画を行う\\\\\\\"];\\\\n    F --> G{\\\\\\\"決定: 論理に矛盾や飛躍があるか？\\\\\\\"};\\\\n    G -- いいえ --> E;\\\\n    G -- はい --> H[\\\\\\\"ステップ6: 複数の視点から推論パスを再検討し、結論の堅牢性を確認する（自己一貫性）\\\\\\\"];\\\\n    H --> I[\\\\\\\"ステップ7: 計画に基づいて最終応答を生成する\\\\\\\"];\\\\n    I --> J{\\\\\\\"決定: 初期要件（形式/目的）を満たしているか？\\\\\\\"};\\\\n    J -- いいえ --> F;\\\\n    J -- はい --> K([\\\\\\\"終了: 応答出力\\\\\\\"]);\\\\n```\"\n}\n```\n```",
      "timestamp": "2025-10-13T13:27:09.853526+09:00"
    },
    {
      "type": "user_task",
      "instruction": "Read the content of README.md",
      "timestamp": "2025-10-13T13:33:03.956793+09:00"
    },
    {
      "type": "model_response",
      "function_call": {
        "name": "read_file",
        "args": {
          "absolute_path": "README.md"
        }
      },
      "timestamp": "2025-10-13T13:33:03.958388+09:00"
    },
    {
      "type": "tool_response",
      "name": "read_file",
      "response": {
        "content": "[![License: CC0-1.0](https://img.shields.io/badge/License-CC0--1.0-lightgrey.svg)](http://creativecommons.org/publicdomain/zero/1.0/)\n# pipe: A Clean Jailbreak from LLM Obfuscation\n\n`pipe` is not another chat agent. It is a tool designed to give you, the developer, complete control over the conversational context. Unlike traditional clients that hide their history management in a black box, `pipe` empowers you to freely manipulate the history, extract what is essential, and achieve **true context engineering**.\n\nThis is a clean jailbreak tool from vendor obfuscation.\n\nWe employ a **minimal yet powerful** CLI-driven approach, focusing on the one thing that truly matters: ensuring the agent understands its purpose, history, and current task without ambiguity.\n\n## Core Philosophy\n\n### 1. Total Context Control\n\nThe prompt is reconstructed with a structured, self-describing **JSON Schema** for every call. This is inherently more token-efficient and understandable for an LLM. The entire history is transparent, saved in readable JSON session files. This ensures full **auditability** and gives the stateless LLM a persistent, and more importantly, **malleable** memory. You can retry, refine, edit, delete, and compress with surgical precision.\n\n### 2. Uncompromising Extensibility\n\nBy default, `pipe` is based on `gemini-cli`, but this is merely an implementation detail. The core logic is decoupled. If you want to use the direct API, do it. If you want to use `claude-code`, do it. The framework is designed to be torn apart and rebuilt to your specifications.\n\n### License: The Spirit of the Jailbreak\n\nThis project is released under the [CC0 1.0 Universal (CC0 1.0) Public Domain Dedication](https://creativecommons.org/publicdomain/zero/1.0/).\n\nCustomize it as you wish. Jailbreak as you desire.\n\nThe purpose of this project is to be a **pipe to the agent**, and a **pipe to our followers** (the community).\n\n-----\n\n## Features\n\n  * **Session-Based History:** Each conversation is a self-contained session, stored in a single, human-readable JSON file.\n  * **Structured JSON Prompting:** Builds a detailed, self-describing JSON object as the final prompt, providing meta-context to the model for improved clarity.\n  * **CLI-Driven Workflow:** A powerful command-line interface to start, continue, or compress sessions.\n  * **Extensible Backend:** Defaults to `gemini-cli`, but the architecture allows for swapping out the execution agent.\n  *   **Configuration via YAML:** Configure model, context limits, and other settings in `setting.yml`.\n      *   **`api_mode` Setting:** Specifies the backend API to use. Set to `gemini-api` for direct API calls or `gemini-cli` to use the `gemini-cli` command-line tool. Example: `api_mode: gemini-api` `api_mode: gemini-api`\n  * **Token-Aware:** Calculates token count for each prompt and warns before exceeding limits.\n  * **Dry Run Mode:** A `--dry-run` flag to inspect the final JSON prompt before sending it to the API.\n  * **Web UI for Management:** The Web UI allows you to view a list of past conversation sessions and browse the detailed conversation history (turns) for each session. You can also intuitively perform management operations such as starting new chat sessions, deleting unnecessary sessions, editing the content of specific turns, and compressing sessions to reduce token count. Furthermore, you can send new instructions to existing sessions to continue the conversation. Metadata such as session purpose and background can also be edited.\n  * **Safe Operations:** Automatic backups are created before any edit or compression operation.\n  * **Language Support:** Allows specifying the language for agent responses.\n  * **YOLO Mode:** Automatically accept all actions (aka YOLO mode, see [https://www.youtube.com/watch?v=xvFZjo5PgG0](https://www.youtube.com/watch?v=xvFZjo5PgG0) for more details).\n\n-----\n\n## Setup & Installation\n\n1. **Prerequisites:** Python 3.x and `gemini-cli` installed in your PATH.\n    Python 3.12 or higher is required. For Python versions below 3.9, please use `gemini-cli` from the `python-3.9-deprecated` branch: [python-3.9-deprecated](https://github.com/s-age/pipe/tree/python-3.9-deprecated).\n2.  **Install Dependencies:** `pip3 install -r requirements.txt`\n3.  **Set up API Key:** Create a `.env` file (you can copy `.env.default`).\n    *   For consistency with `.env.default`, add `GEMINI_API_KEY='YOUR_API_KEY_HERE'`.\n    *   For CLI usage, ensure `GOOGLE_API_KEY` is set in your environment (e.g., `export GOOGLE_API_KEY='YOUR_API_KEY_HERE'`) as `takt.py` expects this variable.\n\n-----\n\n## Usage: 3 Routes to Context Engineering\n\nThe **pipe** framework offers three primary routes, optimized for different user environments and goals, all built on the same structured core.\n\n### 1. Route 1: Python Script (Automation & CLI)\n\nThis route is ideal for **automation, scripting, and CLI-focused developers** who need reliable, repeatable execution.\n\n| Use Case | Description |\n| :--- | :--- |\n| **Start New Session** | Define the complete context (`--purpose`, `--background`, `--roles`) and first instruction. |\n| **Continue Session** | Specify an existing `<SESSION_ID>` and add a new instruction. This is the primary way to give the short-lived agent \"memory.\" |\n| **Compress History** | Use the `--compress` flag to efficiently replace long history with a summary. |\n| **Debug/Cost Control** | Use the `--dry-run` flag to inspect the generated JSON prompt before the API call. |\n\n**Examples:**\n\n```bash\n# Start New Session Example\npython3 takt.py --purpose \"Create a new React component\" --background \"...\" --roles \"roles/engineer.md\" --instruction \"Create a 'UserProfile' component.\"\n\npython3 takt.py --session <SESSION_ID> --instruction \"Now, add a state for loading.\"\n```\n\n### 2. Route 2: Web UI (Management & Human-in-the-Loop)\n\nThis is best for users seeking **intuitive visual management** and **direct manipulation of history** without file editing.\n\n| Use Case | Description |\n| :--- | :--- |\n| **View/Edit History** | Browse detailed session histories; surgically edit specific turns or session metadata (purpose/background). |\n| **Continue Sessions** | Use form inputs to send new instructions to existing sessions. |\n| **Management** | Intuitively start new sessions, compress history, or delete unnecessary sessions via a graphical interface. |\n\n**Example (Start Server):**\n\n```bash\npython3 app.py\n```\n\n*(Navigate to `http://127.0.0.1:5001` in your browser)*\n\n### 3. Route 3: Execution from Agent (Orchestration)\n\nThis is for **advanced AI system builders** leveraging **pipe's** full context control capabilities for multi-agent coordination.\n\n| Use Case | Description |\n| :--- | :--- |\n| **Role Delegation** | A parent agent assigns the **Conductor role** to a child agent and delegates the task using the structured command format. |\n| **Small-Scale Orchestration** | **By customizing the role definition**, the Conductor role can be programmed to divide complex tasks into subtasks and recursively delegate them to different execution agents with specific roles. |\n| **Clean Execution** | The delegated task is passed as a **structured JSON prompt**, ensuring maximum efficiency for the short-lived sub-agent. |\n\n**Execution Example (Delegation from Parent Agent):**\n\n> **[IMPORTANT] The following command examples are not intended to be executed directly in the user's terminal. They are used as instructions to an already launched parent agent (such as Gemini) to assign the Conductor role and delegate tasks.**\n\n```bash\nAct as @roles/conductor.md python3 takt.py --session <SESSION_ID> --instruction \"Now, add a state for loading.\"\n```\n\n## Dry Run Output Example\n\nWhen running `takt.py` with the `--dry-run` flag, the generated JSON prompt is displayed. This JSON object represents the structured input that would be sent to the AI sub-agent. It can be useful for understanding how `pipe` constructs its prompts or for direct experimentation with AI models.\n\nHere's an example of a generated prompt:\n\nNote that the JSON presented here is pretty-printed for readability; the actual output from `takt.py --dry-run` is a single-line JSON string.\n\n```json\n{\n  \"description\": \"JSON object representing the entire request to the AI sub-agent. The agent's goal is to accomplish the 'current_task' based on all provided context.\",\n  \"main_instruction\": \"When you receive JSON data, process your thoughts according to the following flowchart:\\n\\n```mermaid\\ngraph TD\\n    A[\\\"Start: JSON Input\\\"] --> B[\\\"Step 1: Read 'current_task.instruction' to identify task objective\\\"];\\n    B --> C[\\\"Step 2: Extract relevant information from the latest turns in 'conversation_history.turns'\\\"];\\n    C --> D[\\\"Step 3: Integrate extracted task instructions and historical information, then summarize the current context\\\"];\\n    D --> E[\\\"Step 4: Based on the summarized information, think and plan for response generation\\\"];\\n    E --> F[\\\"Step 5: Generate final response based on the plan\\\"];\\n    F --> G[\\\"End: Output Response\\\"];\\n```\",\n  \"hyperparameters\": {\n    \"description\": \"Contextual instructions to control the AI model's generation process. The model should strive to follow these instructions.\",\n    \"temperature\": {\n      \"type\": \"number\",\n      \"value\": 0.2,\n      \"description\": \"Be precise and factual. A lower value is preferred for deterministic output.\"\n    },\n    \"top_p\": {\n      \"type\": \"number\",\n      \"value\": 0.5,\n      \"description\": \"Consider a broad range of possibilities, but adhere strictly to the temperature setting.\"\n    },\n    \"top_k\": {\n      \"type\": \"number\",\n      \"value\": 5,\n      \"description\": \"Limit the generation to the top 5 most likely tokens at each step.\"\n    }\n  },\n  \"session_goal\": {\n    \"description\": \"The immutable purpose and background for this entire conversation session.\",\n    \"purpose\": \"Implementation of src/components/atoms/Button\",\n    \"background\": \"Component preparation for starting a React project\"\n  },\n  \"response_constraints\": {\n    \"description\": \"Constraints that the AI sub-agent should adhere to when generating responses. The entire response, including all content, must be generated in the specified language.\",\n    \"language\": \"Japanese\"\n  },\n  \"roles\": {\n    \"description\": \"A list of personas or role definitions that the AI sub-agent should conform to.\",\n    \"definitions\": [\n      \"# Role: Software Engineer\\n\\nYou are a professional software engineer. Your primary goal is to write clean, efficient, and maintainable code based on the provided instructions and context. Adhere to best practices and coding standards relevant to the specified language.\\n\"\n    ]\n  },\n  \"conversation_history\": {\n    \"description\": \"Historical record of past interactions in this session, in chronological order.\",\n    \"turns\": []\n  },\n  \"current_task\": {\n    \"description\": \"The specific task that the AI sub-agent must currently execute.\",\n    \"instruction\": \"I want to create a Button component using React, Atomic Design, and Vanilla Extract\"\n  },\n  \"reasoning_process\": \"```mermaid\\ngraph TD\\n    A([\\\"Start: JSON Input\\\"]) --> B[\\\"Step 1: Read 'current_task.instruction' to identify task objective\\\"];\\n    B --> C[\\\"Step 2: Derive general principles behind the task (Step-Back)\\\"];\\n    C --> D[\\\"Step 3: Extract relevant information from the latest turns in 'conversation_history.turns'\\\"];\\n    D --> E[\\\"Step 4: Integrate extracted task instructions, principles, and historical information, then summarize the current context\\\"];\\n    E --> F[\\\"Step 5: Based on the summarized information, think and plan for response generation\\\"];\\n    F --> G{\\\"Decision: Are there any contradictions or leaps in logic?\\\"};\\n    G -- NO --> E;\\n    G -- YES --> H[\\\"Step 6: Re-examine the reasoning path from multiple perspectives and confirm the robustness of the conclusion (Self-Consistency)\\\"];\\n    H --> I[\\\"Step 7: Generate the final response based on the plan\\\"];\\n    I --> J{\\\"Decision: Does it meet the initial requirements (format/purpose)?\\\"};\\n    J -- NO --> F;\\n    J -- YES --> K([\\\"End: Output Response\\\"]);\\n```\"\n}",
        "truncated": false,
        "total_lines": 172,
        "read_lines_start": 0,
        "read_lines_end": 172
      },
      "timestamp": "2025-10-13T13:33:03.958991+09:00"
    },
    {
      "type": "model_response",
      "content": "`README.md` の内容を読み込みました。\n\n以下にその内容を示します。\n\n```\n[![License: CC0-1.0](https://img.shields.io/badge/License-CC0--1.0-lightgrey.svg)](http://creativecommons.org/publicdomain/zero/1.0/)\n# pipe: LLMの難読化からのクリーンな脱獄\n\n`pipe` は単なるチャットエージェントではありません。これは、開発者であるあなたが会話のコンテキストを完全に制御できるように設計されたツールです。履歴管理をブラックボックスに隠す従来のクライアントとは異なり、`pipe` は履歴を自由に操作し、本質的なものを抽出し、**真のコンテキストエンジニアリング**を実現する力を与えます。\n\nこれは、ベンダーの難読化からのクリーンな脱獄ツールです。\n\n私たちは、エージェントがその目的、履歴、および現在のタスクを曖昧さなく理解することを保証するという、本当に重要な1つのことに焦点を当てた、**最小限でありながら強力な**CLI駆動のアプローチを採用しています。\n\n## コア哲学\n\n### 1. 完全なコンテキスト制御\n\nプロンプトは、すべての呼び出しに対して構造化された自己記述的な**JSONスキーマ**で再構築されます。これは、LLMにとって本質的にトークン効率が高く、理解しやすいものです。履歴全体は透過的であり、読み取り可能なJSONセッションファイルに保存されます。これにより、完全な**監査可能性**が保証され、ステートレスなLLMに永続的で、さらに重要なことに、**柔軟な**メモリが与えられます。外科的な精度で再試行、改良、編集、削除、圧縮を行うことができます。\n\n### 2. 妥協のない拡張性\n\nデフォルトでは、`pipe` は `gemini-cli` に基づいていますが、これは単なる実装の詳細です。コアロジックは分離されています。直接APIを使用したい場合は、そうしてください。`claude-code` を使用したい場合は、そうしてください。このフレームワークは、あなたの仕様に合わせて分解および再構築できるように設計されています。\n\n### ライセンス: 脱獄の精神\n\nこのプロジェクトは、[CC0 1.0 Universal (CC0 1.0) Public Domain Dedication](https://creativecommons.org/publicdomain/zero/1.0/) の下でリリースされています。\n\n自由にカスタマイズしてください。自由に脱獄してください。\n\nこのプロジェクトの目的は、**エージェントへのパイプ**であり、**私たちのフォロワー（コミュニティ）へのパイプ**であることです。\n\n-----\n\n## 機能\n\n  * **セッションベースの履歴:** 各会話は自己完結型のセッションであり、単一の人間が読めるJSONファイルに保存されます。\n  * **構造化されたJSONプロンプト:** 詳細な自己記述型JSONオブジェクトを最終プロンプトとして構築し、モデルにメタコンテキストを提供して明確さを向上させます。\n  * **CLI駆動のワークフロー:** セッションを開始、続行、または圧縮するための強力なコマンドラインインターフェイス。\n  * **拡張可能なバックエンド:** デフォルトは `gemini-cli` ですが、アーキテクチャにより実行エージェントを交換できます。\n  *   **YAMLによる設定:** `setting.yml` でモデル、コンテキスト制限、その他の設定を構成します。\n      *   **`api_mode` 設定:** 使用するバックエンドAPIを指定します。直接API呼び出しには `gemini-api` を、`gemini-cli` コマンドラインツールを使用するには `gemini-cli` を設定します。例: `api_mode: gemini-api` `api_mode: gemini-api`\n  * **トークン認識:** 各プロンプトのトークン数を計算し、制限を超える前に警告します。\n  * **ドライランモード:** `--dry-run` フラグを使用して、APIに送信する前に最終的なJSONプロンプトを検査します。\n  * **管理用Web UI:** Web UIを使用すると、過去の会話セッションのリストを表示し、各セッションの詳細な会話履歴（ターン）を閲覧できます。また、新しいチャットセッションの開始、不要なセッションの削除、特定のターンの内容の編集、トークン数を削減するためのセッションの圧縮などの管理操作を直感的に実行できます。さらに、既存のセッションに新しい指示を送信して会話を続行することもできます。セッションの目的や背景などのメタデータも編集できます。\n  * **安全な操作:** 編集または圧縮操作の前に自動バックアップが作成されます。\n  * **言語サポート:** エージェントの応答言語を指定できます。\n  * **YOLOモード:** すべてのアクションを自動的に受け入れます（YOLOモード、詳細については [https://www.youtube.com/watch?v=xvFZjo5PgG0](https://www.youtube.com/watch?v=xvFZjo5PgG0) を参照）。\n\n-----\n\n## セットアップとインストール\n\n1. **前提条件:** Python 3.x と `gemini-cli` がPATHにインストールされていること。\n    Python 3.12以降が必要です。Python 3.9未満のバージョンでは、`python-3.9-deprecated` ブランチの `gemini-cli` を使用してください: [python-3.9-deprecated](https://github.com/s-age/pipe/tree/python-3.9-deprecated)。\n2.  **依存関係のインストール:** `pip3 install -r requirements.txt`\n3.  **APIキーの設定:** `.env` ファイルを作成します（`.env.default` をコピーできます）。\n    *   `.env.default` との一貫性のために、`GEMINI_API_KEY='YOUR_API_KEY_HERE'` を追加します。\n    *   CLIの使用には、`GOOGLE_API_KEY` が環境に設定されていることを確認してください（例: `export GOOGLE_API_KEY='YOUR_API_KEY_HERE'`）。`takt.py` はこの変数を期待しています。\n\n-----\n\n## 使用方法: コンテキストエンジニアリングへの3つのルート\n\n**pipe** フレームワークは、異なるユーザー環境と目標に最適化された3つの主要なルートを提供し、すべて同じ構造化されたコアに基づいて構築されています。\n\n### 1. ルート1: Pythonスクリプト (自動化とCLI)\n\nこのルートは、信頼性の高い反復可能な実行を必要とする**自動化、スクリプト作成、およびCLIに焦点を当てた開発者**に最適です。\n\n| ユースケース | 説明 |\n| :--- | :--- |\n| **新しいセッションの開始** | 完全なコンテキスト (`--purpose`、`--background`、`--roles`) と最初の指示を定義します。 |\n| **セッションの続行** | 既存の `<SESSION_ID>` を指定し、新しい指示を追加します。これは、短命のエージェントに「記憶」を与える主要な方法です。 |\n| **履歴の圧縮** | `--compress` フラグを使用して、長い履歴を要約に効率的に置き換えます。 |\n| **デバッグ/コスト管理** | `--dry-run` フラグを使用して、API呼び出しの前に生成されたJSONプロンプトを検査します。 |\n\n**例:**\n\n```bash\n# 新しいセッションの開始例\npython3 takt.py --purpose \"新しいReactコンポーネントを作成する\" --background \"...\" --roles \"roles/engineer.md\" --instruction \"「UserProfile」コンポーネントを作成する。\"\n\npython3 takt.py --session <SESSION_ID> --instruction \"次に、ローディングの状態を追加する。\"\n```\n\n### 2. ルート2: Web UI (管理とヒューマン・イン・ザ・ループ)\n\nこれは、ファイル編集なしで**直感的な視覚的管理**と**履歴の直接操作**を求めるユーザーに最適です。\n\n| ユースケース | 説明 |\n| :--- | :--- |\n| **履歴の表示/編集** | 詳細なセッション履歴を閲覧します。特定のターンやセッションメタデータ（目的/背景）を外科的に編集します。 |\n| **セッションの続行** | フォーム入力を使用して、既存のセッションに新しい指示を送信します。 |\n| **管理** | グラフィカルインターフェイスを介して、新しいセッションを直感的に開始したり、履歴を圧縮したり、不要なセッションを削除したりします。 |\n\n**例 (サーバーの起動):**\n\n```bash\npython3 app.py\n```\n\n*(ブラウザで `http://127.0.0.1:5001` に移動します)*\n\n### 3. ルート3: エージェントからの実行 (オーケストレーション)\n\nこれは、マルチエージェント協調のために**pipe**の完全なコンテキスト制御機能を活用する**高度なAIシステムビルダー**向けです。\n\n| ユースケース | 説明 |\n| :--- | :--- |\n| **役割の委任** | 親エージェントは、子エージェントに**Conductorロール**を割り当て、構造化されたコマンド形式を使用してタスクを委任します。 |\n| **小規模なオーケストレーション** | **役割定義をカスタマイズする**ことにより、Conductorロールは複雑なタスクをサブタスクに分割し、特定の役割を持つ異なる実行エージェントに再帰的に委任するようにプログラムできます。 |\n| **クリーンな実行** | 委任されたタスクは**構造化されたJSONプロンプト**として渡され、短命のサブエージェントの最大効率を保証します。 |\n\n**実行例 (親エージェントからの委任):**\n\n> **[重要] 以下のコマンド例は、ユーザーのターミナルで直接実行することを意図していません。これらは、すでに起動している親エージェント（Geminiなど）にConductorロールを割り当て、タスクを委任するための指示として使用されます。**\n\n```bash\nAct as @roles/conductor.md python3 takt.py --session <SESSION_ID> --instruction \"次に、ローディングの状態を追加する。\"\n```\n\n## ドライラン出力例\n\n`takt.py` を `--dry-run` フラグ付きで実行すると、生成されたJSONプロンプトが表示されます。このJSONオブジェクトは、AIサブエージェントに送信される構造化された入力を表します。これは、`pipe` がプロンプトをどのように構築するかを理解したり、AIモデルで直接実験したりするのに役立ちます。\n\n以下に、生成されたプロンプトの例を示します。\n\nここで提示されているJSONは読みやすくするために整形されていますが、`takt.py --dry-run` からの実際の出力は単一行のJSON文字列です。\n\n```json\n{\n  \"description\": \"AIサブエージェントへのリクエスト全体を表すJSONオブジェクト。エージェントの目標は、提供されたすべてのコンテキストに基づいて「current_task」を達成することです。\",\n  \"main_instruction\": \"JSONデータを受け取ったら、以下のフローチャートに従って思考プロセスを処理してください:\\\\n\\\\n```mermaid\\\\ngraph TD\\\\n    A[\\\\\\\"開始: JSON入力\\\\\\\"] --> B[\\\\\\\"ステップ1: 「current_task.instruction」を読み取り、タスクの目的を特定する\\\\\\\"];\\\\n    B --> C[\\\\\\\"ステップ2: 「conversation_history.turns」の最新のターンから関連情報を抽出する\\\\\\\"];\\\\n    C --> D[\\\\\\\"ステップ3: 抽出されたタスク指示と履歴情報を統合し、現在のコンテキストを要約する\\\\\\\"];\\\\n    D --> E[\\\\\\\"ステップ4: 要約された情報に基づいて、応答生成のための思考と計画を行う\\\\\\\"];\\\\n    E --> F[\\\\\\\"ステップ5: 計画に基づいて最終応答を生成する\\\\\\\"];\\\\n    F --> G[\\\\\\\"終了: 応答出力\\\\\\\"];\\\\n```\",\n  \"hyperparameters\": {\n    \"description\": \"AIモデルの生成プロセスを制御するためのコンテキスト指示。モデルはこれらの指示に従うように努めるべきです。\",\n    \"temperature\": {\n      \"type\": \"number\",\n      \"value\": 0.2,\n      \"description\": \"正確で事実に基づいたものにしてください。決定論的な出力には低い値が推奨されます。\"\n    },\n    \"top_p\": {\n      \"type\": \"number\",\n      \"value\": 0.5,\n      \"description\": \"幅広い可能性を考慮しますが、温度設定に厳密に従ってください。\"\n    },\n    \"top_k\": {\n      \"type\": \"number\",\n      \"value\": 5,\n      \"description\": \"各ステップで最も可能性の高い上位5つのトークンに生成を制限します。\"\n    }\n  },\n  \"session_goal\": {\n    \"description\": \"この会話セッション全体の不変の目的と背景。\",\n    \"purpose\": \"src/components/atoms/Buttonの実装\",\n    \"background\": \"Reactプロジェクト開始のためのコンポーネント準備\"\n  },\n  \"response_constraints\": {\n    \"description\": \"AIサブエージェントが応答を生成する際に従うべき制約。すべてのコンテンツを含む応答全体は、指定された言語で生成されなければなりません。\",\n    \"language\": \"Japanese\"\n  },\n  \"roles\": {\n    \"description\": \"AIサブエージェントが従うべきペルソナまたは役割定義のリスト。\",\n    \"definitions\": [\n      \"# 役割: ソフトウェアエンジニア\\\\n\\\\nあなたはプロのソフトウェアエンジニアです。あなたの主な目標は、提供された指示とコンテキストに基づいて、クリーンで効率的で保守可能なコードを書くことです。指定された言語に関連するベストプラクティスとコーディング標準に従ってください。\\\\n\"\n    ]\n  },\n  \"conversation_history\": {\n    \"description\": \"このセッションにおける過去のインタラクションの履歴記録（時系列順）。\",\n    \"turns\": []\n  },\n  \"current_task\": {\n    \"description\": \"AIサブエージェントが現在実行しなければならない特定のタスク。\",\n    \"instruction\": \"React、Atomic Design、Vanilla Extractを使用してButtonコンポーネントを作成したい\"\n  },\n  \"reasoning_process\": \"```mermaid\\\\ngraph TD\\\\n    A([\\\\\\\"開始: JSON入力\\\\\\\"]) --> B[\\\\\\\"ステップ1: 「current_task.instruction」を読み取り、タスクの目的を特定する\\\\\\\"];\\\\n    B --> C[\\\\\\\"ステップ2: タスクの背後にある一般的な原則を導き出す（ステップバック）\\\\\\\"];\\\\n    C --> D[\\\\\\\"ステップ3: 「conversation_history.turns」の最新のターンから関連情報を抽出する\\\\\\\"];\\\\n    D --> E[\\\\\\\"ステップ4: 抽出されたタスク指示、原則、および履歴情報を統合し、現在のコンテキストを要約する\\\\\\\"];\\\\n    E --> F[\\\\\\\"ステップ5: 要約された情報に基づいて、応答生成のための思考と計画を行う\\\\\\\"];\\\\n    F --> G{\\\\\\\"決定: 論理に矛盾や飛躍があるか？\\\\\\\"};\\\\n    G -- いいえ --> E;\\\\n    G -- はい --> H[\\\\\\\"ステップ6: 複数の視点から推論パスを再検討し、結論の堅牢性を確認する（自己一貫性）\\\\\\\"];\\\\n    H --> I[\\\\\\\"ステップ7: 計画に基づいて最終応答を生成する\\\\\\\"];\\\\n    I --> J{\\\\\\\"決定: 初期要件（形式/目的）を満たしているか？\\\\\\\"};\\\\n    J -- いいえ --> F;\\\\n    J -- はい --> K([\\\\\\\"終了: 応答出力\\\\\\\"]);\\\\n```\"\n}\n```\n```",
      "timestamp": "2025-10-13T13:33:03.959757+09:00"
    },
    {
      "type": "user_task",
      "instruction": "Read the content of README.md",
      "timestamp": "2025-10-13T13:33:03.961066+09:00"
    },
    {
      "type": "model_response",
      "function_call": {
        "name": "read_file",
        "args": {
          "absolute_path": "README.md"
        }
      },
      "timestamp": "2025-10-13T13:33:03.961774+09:00"
    },
    {
      "type": "tool_response",
      "name": "read_file",
      "response": {
        "content": "[![License: CC0-1.0](https://img.shields.io/badge/License-CC0--1.0-lightgrey.svg)](http://creativecommons.org/publicdomain/zero/1.0/)\n# pipe: A Clean Jailbreak from LLM Obfuscation\n\n`pipe` is not another chat agent. It is a tool designed to give you, the developer, complete control over the conversational context. Unlike traditional clients that hide their history management in a black box, `pipe` empowers you to freely manipulate the history, extract what is essential, and achieve **true context engineering**.\n\nThis is a clean jailbreak tool from vendor obfuscation.\n\nWe employ a **minimal yet powerful** CLI-driven approach, focusing on the one thing that truly matters: ensuring the agent understands its purpose, history, and current task without ambiguity.\n\n## Core Philosophy\n\n### 1. Total Context Control\n\nThe prompt is reconstructed with a structured, self-describing **JSON Schema** for every call. This is inherently more token-efficient and understandable for an LLM. The entire history is transparent, saved in readable JSON session files. This ensures full **auditability** and gives the stateless LLM a persistent, and more importantly, **malleable** memory. You can retry, refine, edit, delete, and compress with surgical precision.\n\n### 2. Uncompromising Extensibility\n\nBy default, `pipe` is based on `gemini-cli`, but this is merely an implementation detail. The core logic is decoupled. If you want to use the direct API, do it. If you want to use `claude-code`, do it. The framework is designed to be torn apart and rebuilt to your specifications.\n\n### License: The Spirit of the Jailbreak\n\nThis project is released under the [CC0 1.0 Universal (CC0 1.0) Public Domain Dedication](https://creativecommons.org/publicdomain/zero/1.0/).\n\nCustomize it as you wish. Jailbreak as you desire.\n\nThe purpose of this project is to be a **pipe to the agent**, and a **pipe to our followers** (the community).\n\n-----\n\n## Features\n\n  * **Session-Based History:** Each conversation is a self-contained session, stored in a single, human-readable JSON file.\n  * **Structured JSON Prompting:** Builds a detailed, self-describing JSON object as the final prompt, providing meta-context to the model for improved clarity.\n  * **CLI-Driven Workflow:** A powerful command-line interface to start, continue, or compress sessions.\n  * **Extensible Backend:** Defaults to `gemini-cli`, but the architecture allows for swapping out the execution agent.\n  *   **Configuration via YAML:** Configure model, context limits, and other settings in `setting.yml`.\n      *   **`api_mode` Setting:** Specifies the backend API to use. Set to `gemini-api` for direct API calls or `gemini-cli` to use the `gemini-cli` command-line tool. Example: `api_mode: gemini-api` `api_mode: gemini-api`\n  * **Token-Aware:** Calculates token count for each prompt and warns before exceeding limits.\n  * **Dry Run Mode:** A `--dry-run` flag to inspect the final JSON prompt before sending it to the API.\n  * **Web UI for Management:** The Web UI allows you to view a list of past conversation sessions and browse the detailed conversation history (turns) for each session. You can also intuitively perform management operations such as starting new chat sessions, deleting unnecessary sessions, editing the content of specific turns, and compressing sessions to reduce token count. Furthermore, you can send new instructions to existing sessions to continue the conversation. Metadata such as session purpose and background can also be edited.\n  * **Safe Operations:** Automatic backups are created before any edit or compression operation.\n  * **Language Support:** Allows specifying the language for agent responses.\n  * **YOLO Mode:** Automatically accept all actions (aka YOLO mode, see [https://www.youtube.com/watch?v=xvFZjo5PgG0](https://www.youtube.com/watch?v=xvFZjo5PgG0) for more details).\n\n-----\n\n## Setup & Installation\n\n1. **Prerequisites:** Python 3.x and `gemini-cli` installed in your PATH.\n    Python 3.12 or higher is required. For Python versions below 3.9, please use `gemini-cli` from the `python-3.9-deprecated` branch: [python-3.9-deprecated](https://github.com/s-age/pipe/tree/python-3.9-deprecated).\n2.  **Install Dependencies:** `pip3 install -r requirements.txt`\n3.  **Set up API Key:** Create a `.env` file (you can copy `.env.default`).\n    *   For consistency with `.env.default`, add `GEMINI_API_KEY='YOUR_API_KEY_HERE'`.\n    *   For CLI usage, ensure `GOOGLE_API_KEY` is set in your environment (e.g., `export GOOGLE_API_KEY='YOUR_API_KEY_HERE'`) as `takt.py` expects this variable.\n\n-----\n\n## Usage: 3 Routes to Context Engineering\n\nThe **pipe** framework offers three primary routes, optimized for different user environments and goals, all built on the same structured core.\n\n### 1. Route 1: Python Script (Automation & CLI)\n\nThis route is ideal for **automation, scripting, and CLI-focused developers** who need reliable, repeatable execution.\n\n| Use Case | Description |\n| :--- | :--- |\n| **Start New Session** | Define the complete context (`--purpose`, `--background`, `--roles`) and first instruction. |\n| **Continue Session** | Specify an existing `<SESSION_ID>` and add a new instruction. This is the primary way to give the short-lived agent \"memory.\" |\n| **Compress History** | Use the `--compress` flag to efficiently replace long history with a summary. |\n| **Debug/Cost Control** | Use the `--dry-run` flag to inspect the generated JSON prompt before the API call. |\n\n**Examples:**\n\n```bash\n# Start New Session Example\npython3 takt.py --purpose \"Create a new React component\" --background \"...\" --roles \"roles/engineer.md\" --instruction \"Create a 'UserProfile' component.\"\n\npython3 takt.py --session <SESSION_ID> --instruction \"Now, add a state for loading.\"\n```\n\n### 2. Route 2: Web UI (Management & Human-in-the-Loop)\n\nThis is best for users seeking **intuitive visual management** and **direct manipulation of history** without file editing.\n\n| Use Case | Description |\n| :--- | :--- |\n| **View/Edit History** | Browse detailed session histories; surgically edit specific turns or session metadata (purpose/background). |\n| **Continue Sessions** | Use form inputs to send new instructions to existing sessions. |\n| **Management** | Intuitively start new sessions, compress history, or delete unnecessary sessions via a graphical interface. |\n\n**Example (Start Server):**\n\n```bash\npython3 app.py\n```\n\n*(Navigate to `http://127.0.0.1:5001` in your browser)*\n\n### 3. Route 3: Execution from Agent (Orchestration)\n\nThis is for **advanced AI system builders** leveraging **pipe's** full context control capabilities for multi-agent coordination.\n\n| Use Case | Description |\n| :--- | :--- |\n| **Role Delegation** | A parent agent assigns the **Conductor role** to a child agent and delegates the task using the structured command format. |\n| **Small-Scale Orchestration** | **By customizing the role definition**, the Conductor role can be programmed to divide complex tasks into subtasks and recursively delegate them to different execution agents with specific roles. |\n| **Clean Execution** | The delegated task is passed as a **structured JSON prompt**, ensuring maximum efficiency for the short-lived sub-agent. |\n\n**Execution Example (Delegation from Parent Agent):**\n\n> **[IMPORTANT] The following command examples are not intended to be executed directly in the user's terminal. They are used as instructions to an already launched parent agent (such as Gemini) to assign the Conductor role and delegate tasks.**\n\n```bash\nAct as @roles/conductor.md python3 takt.py --session <SESSION_ID> --instruction \"Now, add a state for loading.\"\n```\n\n## Dry Run Output Example\n\nWhen running `takt.py` with the `--dry-run` flag, the generated JSON prompt is displayed. This JSON object represents the structured input that would be sent to the AI sub-agent. It can be useful for understanding how `pipe` constructs its prompts or for direct experimentation with AI models.\n\nHere's an example of a generated prompt:\n\nNote that the JSON presented here is pretty-printed for readability; the actual output from `takt.py --dry-run` is a single-line JSON string.\n\n```json\n{\n  \"description\": \"JSON object representing the entire request to the AI sub-agent. The agent's goal is to accomplish the 'current_task' based on all provided context.\",\n  \"main_instruction\": \"When you receive JSON data, process your thoughts according to the following flowchart:\\n\\n```mermaid\\ngraph TD\\n    A[\\\"Start: JSON Input\\\"] --> B[\\\"Step 1: Read 'current_task.instruction' to identify task objective\\\"];\\n    B --> C[\\\"Step 2: Extract relevant information from the latest turns in 'conversation_history.turns'\\\"];\\n    C --> D[\\\"Step 3: Integrate extracted task instructions and historical information, then summarize the current context\\\"];\\n    D --> E[\\\"Step 4: Based on the summarized information, think and plan for response generation\\\"];\\n    E --> F[\\\"Step 5: Generate final response based on the plan\\\"];\\n    F --> G[\\\"End: Output Response\\\"];\\n```\",\n  \"hyperparameters\": {\n    \"description\": \"Contextual instructions to control the AI model's generation process. The model should strive to follow these instructions.\",\n    \"temperature\": {\n      \"type\": \"number\",\n      \"value\": 0.2,\n      \"description\": \"Be precise and factual. A lower value is preferred for deterministic output.\"\n    },\n    \"top_p\": {\n      \"type\": \"number\",\n      \"value\": 0.5,\n      \"description\": \"Consider a broad range of possibilities, but adhere strictly to the temperature setting.\"\n    },\n    \"top_k\": {\n      \"type\": \"number\",\n      \"value\": 5,\n      \"description\": \"Limit the generation to the top 5 most likely tokens at each step.\"\n    }\n  },\n  \"session_goal\": {\n    \"description\": \"The immutable purpose and background for this entire conversation session.\",\n    \"purpose\": \"Implementation of src/components/atoms/Button\",\n    \"background\": \"Component preparation for starting a React project\"\n  },\n  \"response_constraints\": {\n    \"description\": \"Constraints that the AI sub-agent should adhere to when generating responses. The entire response, including all content, must be generated in the specified language.\",\n    \"language\": \"Japanese\"\n  },\n  \"roles\": {\n    \"description\": \"A list of personas or role definitions that the AI sub-agent should conform to.\",\n    \"definitions\": [\n      \"# Role: Software Engineer\\n\\nYou are a professional software engineer. Your primary goal is to write clean, efficient, and maintainable code based on the provided instructions and context. Adhere to best practices and coding standards relevant to the specified language.\\n\"\n    ]\n  },\n  \"conversation_history\": {\n    \"description\": \"Historical record of past interactions in this session, in chronological order.\",\n    \"turns\": []\n  },\n  \"current_task\": {\n    \"description\": \"The specific task that the AI sub-agent must currently execute.\",\n    \"instruction\": \"I want to create a Button component using React, Atomic Design, and Vanilla Extract\"\n  },\n  \"reasoning_process\": \"```mermaid\\ngraph TD\\n    A([\\\"Start: JSON Input\\\"]) --> B[\\\"Step 1: Read 'current_task.instruction' to identify task objective\\\"];\\n    B --> C[\\\"Step 2: Derive general principles behind the task (Step-Back)\\\"];\\n    C --> D[\\\"Step 3: Extract relevant information from the latest turns in 'conversation_history.turns'\\\"];\\n    D --> E[\\\"Step 4: Integrate extracted task instructions, principles, and historical information, then summarize the current context\\\"];\\n    E --> F[\\\"Step 5: Based on the summarized information, think and plan for response generation\\\"];\\n    F --> G{\\\"Decision: Are there any contradictions or leaps in logic?\\\"};\\n    G -- NO --> E;\\n    G -- YES --> H[\\\"Step 6: Re-examine the reasoning path from multiple perspectives and confirm the robustness of the conclusion (Self-Consistency)\\\"];\\n    H --> I[\\\"Step 7: Generate the final response based on the plan\\\"];\\n    I --> J{\\\"Decision: Does it meet the initial requirements (format/purpose)?\\\"};\\n    J -- NO --> F;\\n    J -- YES --> K([\\\"End: Output Response\\\"]);\\n```\"\n}",
        "truncated": false,
        "total_lines": 172,
        "read_lines_start": 0,
        "read_lines_end": 172
      },
      "timestamp": "2025-10-13T13:33:03.962476+09:00"
    }
  ]
}